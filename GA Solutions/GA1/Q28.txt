python streaming_llm_api.py --server

answer:
http://localhost:8080/v1/chat/completions

https://streaming-llm-api.vercel.app/